# GPU-Optimized Machine Learning Code Repository

This repository contains scripts, Jupyter notebooks, and scripts containing function libraries. This repository is designed to facilitate data preprocessing, model training, and testing for time series forecasting tasks with parametrized quantum circuits. The code is optimized for GPU acceleration using the Jax library.

## Contents

### Data Preprocessing

- `DataPreprocessing.py`: Python script for data preprocessing.
- `DataPreprocessing.ipynb`: Jupyter notebook for data preprocessing.

### Training

- `Training.py`: Python script for training machine learning models.
- `Training.ipynb`: Jupyter notebook for training machine learning models.

### Testing

- `Testing.py`: Python script for testing trained models.
- `Testing.ipynb`: Jupyter notebook for testing trained models.

### Function Libraries

#### DataPreprocessingFuncs

- `DataPreprocessingFuncs.py`: Python script containing functions for data preprocessing.

#### TrainingFuncs

- `TrainingFuncs.py`: Python script containing functions for training machine learning models.

#### TestingFuncs

- `TestingFuncs.py`: Python script containing functions for testing trained models.

### Results

The `results` folder contains subfolders that store the results obtained from the Training and Testing stages. Each subfolder is named according to the dataset, architecture, and hyperparameters used in training and testing. This structure ensures reusability and facilitates interpretation of the outcomes.

## Contact

If you have any questions, suggestions, or feedback, please feel free to contact Andrew Spiro at andrew.charles.spiro@cern.ch. 
